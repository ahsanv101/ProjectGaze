{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bze9kachzXS_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bze9kachzXS_",
    "outputId": "951ad088-5a87-42e2-d8f8-1cf9d14712d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/homebrew/Cellar/jupyterlab/3.6.1/libexec/lib/python3.11/site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/homebrew/Cellar/jupyterlab/3.6.1/libexec/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Cellar/jupyterlab/3.6.1/libexec/lib/python3.11/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/homebrew/Cellar/jupyterlab/3.6.1/libexec/lib/python3.11/site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/opt/six/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pip in /opt/homebrew/Cellar/jupyterlab/3.6.1/libexec/lib/python3.11/site-packages (23.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in /opt/homebrew/Cellar/jupyterlab/3.6.1/libexec/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/homebrew/Cellar/jupyterlab/3.6.1/libexec/lib/python3.11/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/Cellar/jupyterlab/3.6.1/libexec/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/Cellar/jupyterlab/3.6.1/libexec/lib/python3.11/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Cellar/jupyterlab/3.6.1/libexec/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: PyPDF2 in /opt/homebrew/Cellar/jupyterlab/3.6.1/libexec/lib/python3.11/site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement ntlk (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for ntlk\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/macuser/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/macuser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "%pip install pandas\n",
    "from pandas import *\n",
    "\n",
    "\n",
    "%pip install --upgrade pip\n",
    "%pip install --upgrade nltk\n",
    "\n",
    "%pip install PyPDF2\n",
    "%pip install ntlk\n",
    "import os\n",
    "\n",
    "\n",
    "# # importing required modules\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "import nltk \n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a67a9031",
   "metadata": {
    "id": "a67a9031"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/DHDK/INFO-VIZ/infovizrepo/ProjectGaze/Data/scripts/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# iterate over files in\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# that directory\u001b[39;00m\n\u001b[1;32m     36\u001b[0m files \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m     39\u001b[0m         files\u001b[38;5;241m.\u001b[39mappend(filename\u001b[38;5;241m.\u001b[39mpath)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/DHDK/INFO-VIZ/infovizrepo/ProjectGaze/Data/scripts/'"
     ]
    }
   ],
   "source": [
    "\n",
    "# # importing required modules\n",
    "# from PyPDF2 import PdfReader\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "# import nltk \n",
    "# # nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "part = wn.synsets('body_part')[0]\n",
    "\n",
    "# https://stackoverflow.com/questions/73117109/how-to-identify-body-part-names-in-a-text-with-python\n",
    "def is_body_part(candidate):\n",
    "    for ss in wn.synsets(candidate):\n",
    "        # only get those where the synset matches exactly\n",
    "        name = ss.name().split(\".\", 1)[0]\n",
    "        if name != candidate:\n",
    "            continue\n",
    "        hit = part.lowest_common_hypernyms(ss)\n",
    "        if hit and hit[0] == part:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "import os\n",
    " \n",
    "# assign directory\n",
    "directory = '/content/drive/My Drive/DHDK/INFO-VIZ/infovizrepo/ProjectGaze/Data/scripts/'\n",
    " \n",
    "# iterate over files in\n",
    "# that directory\n",
    "\n",
    "files = []\n",
    "for filename in os.scandir(directory):\n",
    "    if filename.is_file():\n",
    "        files.append(filename.path)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nHY5CwTyzUNo",
   "metadata": {
    "id": "nHY5CwTyzUNo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9f6fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50e9f6fb",
    "outputId": "15321d50-5a92-48e5-b77b-54bcd0444865"
   },
   "outputs": [],
   "source": [
    "def get_title(file_name):\n",
    "    title = file_name.split(\"/\")[-1]\n",
    "    # title = title.replace(\" \",\"\")\n",
    "    # title = title.replace(\"-\",\"\")\n",
    "    title = title.replace(\".pdf\",\"\")\n",
    "    title = title.replace(\".php\",\"\")\n",
    "    \n",
    "    return title\n",
    "# # getting a specific page from the pdf file\n",
    "# page = reader.pages[0]\n",
    " \n",
    "# # extracting text from page\n",
    "# text = page.extract_text()\n",
    "\n",
    "# creating a pdf reader object\n",
    "\n",
    "\n",
    " \n",
    "# printing number of pages in pdf file\n",
    "# print(len(reader.pages))\n",
    "\n",
    "\n",
    "words_0= ['body', 'lips', 'beauty', 'age', 'smile', 'pants', 'skirt', 'dress', 'shirt', 'glow', 'shorts', 'hand','face','finger', 'throat','neck','hair','skin','arm','figure','shoulder'] \n",
    "adj_0=['beautiful', 'gorgeous', 'cute', 'pretty', 'devoted', 'lawful','housewife', 'dumb', 'ignorant', 'silly', 'fragile', 'frightening', 'enchanting', 'stunning','toned', 'breathtaking', 'breath-taking', 'sultry','divine', 'perfect']\n",
    "\n",
    "#second pair of lists with sexualised language\n",
    "words_1=['ass', 'buxom','boob', 'boobs', 'breast', 'breasts','thighs', 'bottom', 'curves', 'underwear', 'panty', 'stockings', 'panties', 'lingerie', 'bra', 'nipple','vagina','cunt','']\n",
    "adj_1= ['seductive','sexy','trashy', 'nude', 'promiscuous', 'sexual', 'hot', 'hottie', 'erotic','fuck-me', 'fuck me','juicy']\n",
    "\n",
    "\n",
    "keywords = ['body','boob','chest','bosom','thighs','thigh','waist','breast','ass','lips',\n",
    "                                                                                'beautiful','attractive','hottie','hot','skirt','beauty','sexy','age',\n",
    "                                                                                'blond','smile','dumb','cute','perfect','buxom','dress','fit','tight',\n",
    "                                                                                'blonde','natural','bra','underwear','panty','panties','divine',\n",
    "                                                                                'curve','curves','trashy','nude','toned','fragile','pretty','womanhood',\n",
    "                                                                                'breathtaking','fuck me eyes','erotic','lingerie','erotic','glow','gorgeous',\n",
    "                                                                                'banging','seductive','promiscuous','enchanting',\n",
    "                                                                                'figure','fuck-me','fuck me','sexual','pants','shirt','dress','shorts']\n",
    "\n",
    "\n",
    "movies_dict={}\n",
    "movies_dict_1={}\n",
    "\n",
    "\n",
    "for f in files:\n",
    "    movie_title = get_title(f)\n",
    "    print(\"--------------------------------------------\", movie_title, \"-----------------------------------------\")\n",
    "\n",
    "    reader = PdfReader(f)\n",
    "    lst=[]\n",
    "    # len(reader.pages)\n",
    "    for i in range(0,len(reader.pages)):\n",
    "        page = reader.pages[i]\n",
    "        text = page.extract_text()\n",
    "        lst.append(text)\n",
    "    #     print(text)\n",
    "    #     print(\"-----------------\")\n",
    "    # len(lst[0])\n",
    "\n",
    "    \n",
    "    word_counts={}\n",
    "    word_counts_1={}\n",
    "    \n",
    "    check = [\"she\", \"her\", \"woman\", \"woman's\", \"women\", \"women's\", \"she's\"]\n",
    "\n",
    "    for i in lst:\n",
    "        tokens = word_tokenize(i)\n",
    "    #     for j in i:\n",
    "    #         tokens = word_tokenize(j) \n",
    "    #         print(tokens)\n",
    "        for k in range(0,len(tokens)):\n",
    "            #lemmatize\n",
    "            tokens[k] = lemmatizer.lemmatize(tokens[k])\n",
    "    #         print(tokens[k].lower())\n",
    "    \n",
    "            if is_body_part(tokens[k].lower()) == True or tokens[k].lower() in words_0+adj_0:\n",
    "                \n",
    "                gram2 = tokens[k-2].lower()\n",
    "                gram1 = tokens[k-1].lower()\n",
    "                gram = tokens[k].lower()\n",
    "                if k+1 in range(-len(tokens), len(tokens)):\n",
    "                  gram0 = tokens[k+1].lower()\n",
    "\n",
    "                if k-3 in range(-len(tokens), len(tokens)):\n",
    "                  gram3 = tokens[k-3].lower()\n",
    "\n",
    "                if gram3 in check or gram2 in check or gram1 in check or gram0 in check:\n",
    "                    print(gram3,gram2,gram1,gram,gram0)\n",
    "\n",
    "            \n",
    "            \n",
    "                    if tokens[k].lower() in word_counts:\n",
    "                        # print(tokens[k], gram0)\n",
    "                        word_counts[tokens[k].lower()] += 1\n",
    "                    else:\n",
    "                        word_counts[tokens[k].lower()] = 1\n",
    "\n",
    "            if tokens[k].lower() in words_1+adj_1:\n",
    "                if k+1 in range(-len(tokens), len(tokens)):\n",
    "                  gram0 = tokens[k+1].lower()\n",
    "\n",
    "                if k-3 in range(-len(tokens), len(tokens)):\n",
    "                  gram3 = tokens[k-3].lower()\n",
    "                \n",
    "                gram2 = tokens[k-2].lower()\n",
    "                gram1 = tokens[k-1].lower()\n",
    "                gram0 = tokens[k].lower()\n",
    "#                 check = ('his','he','man','men',\"man's\",\"men's\",'her','she','their','woman','women', \"woman's\",\"women's\")\n",
    "\n",
    "                if gram3 in check or gram2 in check or gram1 in check or gram0 in check:\n",
    "            \n",
    "                    if tokens[k].lower() in word_counts_1:\n",
    "                        # print(tokens[k], gram0)\n",
    "                        word_counts_1[tokens[k].lower()] += 1\n",
    "                    else:\n",
    "                        word_counts_1[tokens[k].lower()] = 1\n",
    "                    print('dubious',gram2,gram1,gram0)\n",
    "                    \n",
    "    movies_dict[movie_title] = word_counts\n",
    "    movies_dict_1[movie_title] = word_counts_1\n",
    "    \n",
    "    #make sure movie names are the primary keys     \n",
    "print(movies_dict,\"------------------------------------------------------------------------------------\", movies_dict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LrJDgOOv30NL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LrJDgOOv30NL",
    "outputId": "c9e3abad-d6d4-44da-fccc-b67ec87effb5"
   },
   "outputs": [],
   "source": [
    "\n",
    "body_descriptions = DataFrame.from_dict(movies_dict, orient=\"index\")\n",
    "print(body_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36ba9f1-8666-4dcb-b93b-a708d24e45fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "b36ba9f1-8666-4dcb-b93b-a708d24e45fc",
    "outputId": "5ed5a5a4-7486-4332-96f5-f7af7b554129"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a187ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "id": "b3a187ec",
    "outputId": "7ea4882f-63ea-4ef4-d6d7-98a51771b946"
   },
   "outputs": [],
   "source": [
    "print(psycho_naughty)\n",
    "naughty_descriptions = DataFrame.from_dict(movies_dict_1, orient=\"index\")\n",
    "naughty_descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514677f3",
   "metadata": {
    "id": "514677f3"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ad603-0122-42ef-9e69-48fb2860dce1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "469ad603-0122-42ef-9e69-48fb2860dce1",
    "outputId": "18529ff6-b73b-4187-d16a-10cee90d1427"
   },
   "outputs": [],
   "source": [
    "#get general descriptions - n-grams\n",
    "\n",
    "\n",
    "words_0= ['body', 'lips', 'beauty', 'age', 'smile', 'pants', 'skirt', 'dress', 'shirt', 'glow', 'shorts','clothes', 'jacket'] \n",
    "adj_0=['beautiful', 'handsome', 'muscular', 'gorgeous', 'cute', 'confused', 'sad','pretty', 'devoted', 'lawful','housewife', 'dumb', 'ignorant', 'silly', 'smart','thinking','stunned','mysterious', 'enchanting', 'stunning','toned', 'breathtaking', 'breath-taking', 'divine', 'perfect']\n",
    "\n",
    "\n",
    "words_prob=['ass', 'buxom','boob', 'boobs', 'breast', 'breasts','thighs', 'bottom', 'curves', 'underwear', 'panty', 'stockings', 'panties', 'lingerie', 'bra', 'nipple']\n",
    "adj_prob= ['seductive','sexy','trashy', 'nude', 'promiscuous', 'sexual', 'hot', 'hottie', 'erotic','fuck-me', 'fuck me','juicy']\n",
    "\n",
    "\n",
    "movies_dict_men={}\n",
    "movies_dict_women={}\n",
    "\n",
    "\n",
    "movies_dict_men_1={}\n",
    "movies_dict_women_1={}\n",
    "\n",
    "\n",
    "#all data summed up\n",
    "\n",
    "movies_tot={}\n",
    "movies_total_f={}\n",
    "movies_total_m={}\n",
    "movies_total_f_1={}\n",
    "movies_total_m_1={}\n",
    "\n",
    "for f in files:\n",
    "    movie_title = get_title(f)\n",
    "    print(\"--------------------------------------------\", movie_title, \"-----------------------------------------\")\n",
    "\n",
    "\n",
    "    reader = PdfReader(f)\n",
    "    lst=[]\n",
    "    # len(reader.pages)\n",
    "    for i in range(0,len(reader.pages)):\n",
    "        page = reader.pages[i]\n",
    "        text = page.extract_text()\n",
    "        lst.append(text)\n",
    "    #     print(text)\n",
    "    #     print(\"-----------------\")\n",
    "    # len(lst[0])\n",
    "\n",
    "    \n",
    "    word_counts_men={}\n",
    "    word_counts_women={}\n",
    "    word_counts_men_1 = {}\n",
    "    word_counts_women_1 = {}\n",
    "\n",
    "    \n",
    "    \n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    for i in lst:\n",
    "        tokens = word_tokenize(i)\n",
    "    #     for j in i:\n",
    "    #         tokens = word_tokenize(j) \n",
    "    #         print(tokens)\n",
    "        for k in range(0,len(tokens)):\n",
    "            #lemmatize\n",
    "            tokens[k] = lemmatizer.lemmatize(tokens[k])\n",
    "    #         print(tokens[k].lower())\n",
    "\n",
    "\n",
    "#                 check = ('his','he','man','men',\"man's\",\"men's\",'her','she','their','woman','women', \"woman's\",\"women's\")\n",
    "\n",
    "            check_women = ['her','she','woman','women', \"woman's\",\"women's\"]\n",
    "            check_men = ['his','he','man','men',\"man's\",\"men's\"]\n",
    "\n",
    "            if is_body_part(tokens[k].lower()) == True or tokens[k].lower() in words_0+adj_0:\n",
    "              if k+1 in range(-len(tokens), len(tokens)):\n",
    "                gram0 = tokens[k+1].lower()\n",
    "\n",
    "              if k-3 in range(-len(tokens), len(tokens)):\n",
    "                gram3 = tokens[k-3].lower()\n",
    "\n",
    "      \n",
    "              if k-2 in range(-len(tokens), len(tokens)):\n",
    "                  \n",
    "                  gram2 = tokens[k-2].lower()\n",
    "              \n",
    "              if k-1 in range(-len(tokens), len(tokens)):\n",
    "                  gram1 = tokens[k-1].lower()\n",
    "                  gram = tokens[k].lower()\n",
    "\n",
    "    #CHECK WOMEN\n",
    "\n",
    "              if gram3 in check_women or gram2 in check_women or gram1 in check_women or gram0 in check_women:\n",
    "                  print(gram2,gram1,gram,gram0)\n",
    "\n",
    "\n",
    "              \n",
    "                  if tokens[k].lower() in word_counts_women:\n",
    "                      # print(tokens[k], gram0)\n",
    "                      word_counts_women[tokens[k].lower()] += 1\n",
    "                      movies_total_f[tokens[k].lower()] += 1\n",
    "\n",
    "                  else:\n",
    "                      word_counts_women[tokens[k].lower()] = 1\n",
    "                      movies_total_f[tokens[k].lower()] = 1\n",
    "\n",
    "\n",
    "        \n",
    "                \n",
    "#                 check = ('his','he','man','men',\"man's\",\"men's\",'her','she','their','woman','women', \"woman's\",\"women's\")\n",
    "#CHECK MEN\n",
    "            \n",
    "            # if is_body_part(tokens[k].lower()) == True or tokens[k].lower() in words_0+adj_0:\n",
    "\n",
    "              if gram3 in check_men or gram2 in check_men or gram1 in check_men or gram0 in check_men:\n",
    "              \n",
    "                  if tokens[k].lower() in word_counts_men:\n",
    "                      # print(tokens[k], gram0)\n",
    "                      word_counts_men[tokens[k].lower()] += 1\n",
    "                      movies_total_m[tokens[k].lower()] += 1\n",
    "\n",
    "                  else:\n",
    "                      word_counts_men[tokens[k].lower()] = 1\n",
    "                      movies_total_m[tokens[k].lower()] = 1\n",
    "\n",
    "                  print(\"men\",gram2,gram1,gram,gram0)\n",
    "\n",
    "            \n",
    "   #========================================!!!!!!!!!!! FIND INAPPROPRIATE SPEECH !!!!!!!!!!!!!!!!!!!!!!!!!!==============================     \n",
    "\n",
    "\n",
    "            if tokens[k].lower() in words_1+adj_1:\n",
    "              if k+1 in range(-len(tokens), len(tokens)):\n",
    "                gram0 = tokens[k+1].lower()\n",
    "\n",
    "              if k-3 in range(-len(tokens), len(tokens)):\n",
    "                gram3 = tokens[k-3].lower()\n",
    "\n",
    "      \n",
    "              if k-2 in range(-len(tokens), len(tokens)):\n",
    "                  \n",
    "                  gram2 = tokens[k-2].lower()\n",
    "              \n",
    "              if k-1 in range(-len(tokens), len(tokens)):\n",
    "                  gram1 = tokens[k-1].lower()\n",
    "                  gram = tokens[k].lower()\n",
    "\n",
    "    #CHECK WOMEN\n",
    "\n",
    "              if gram3 in check_women or gram2 in check_women or gram1 in check_women or gram0 in check_women:\n",
    "                  print(gram2,gram1,gram,gram0)\n",
    "\n",
    "\n",
    "              \n",
    "                  if tokens[k].lower() in word_counts_women_1:\n",
    "                      # print(tokens[k], gram0)\n",
    "                      word_counts_women_1[tokens[k].lower()] += 1\n",
    "                      movies_total_f_1[tokens[k].lower()] += 1\n",
    "\n",
    "                  else:\n",
    "                      word_counts_women_1[tokens[k].lower()] = 1\n",
    "                      movies_total_f_1[tokens[k].lower()] = 1\n",
    "\n",
    "\n",
    "                  \n",
    "  #                 check = ('his','he','man','men',\"man's\",\"men's\",'her','she','their','woman','women', \"woman's\",\"women's\")\n",
    "  #CHECK MEN\n",
    "              \n",
    "              # if is_body_part(tokens[k].lower()) == True or tokens[k].lower() in words_0+adj_0:\n",
    "\n",
    "              if gram3 in check_men or gram2 in check_men or gram1 in check_men or gram0 in check_men:\n",
    "              \n",
    "                  if tokens[k].lower() in word_counts_men_1:\n",
    "                      # print(tokens[k], gram0)\n",
    "                      word_counts_men_1[tokens[k].lower()] += 1\n",
    "                      movies_total_m_1[tokens[k].lower()] += 1\n",
    "\n",
    "                  else:\n",
    "                      word_counts_men_1[tokens[k].lower()] = 1\n",
    "                      movies_total_m_1[tokens[k].lower()] = 1\n",
    "\n",
    "                  print(\"men_1\",gram2,gram1,gram,gram0)\n",
    "      \n",
    "          \n",
    "  # print(movies_dict_women,\"------------------------------------------------------------------------------------\", movies_dict_men)\n",
    "        \n",
    "    movies_dict_women[movie_title] = word_counts_women\n",
    "    movies_dict_men[movie_title] = word_counts_men\n",
    "    movies_tot[\"males\"]=movies_total_m\n",
    "    movies_tot[\"females\"]=movies_total_f\n",
    "\n",
    "    #fill inappropriate\n",
    "\n",
    "    movies_dict_women_1[movie_title] = word_counts_women_1\n",
    "    movies_dict_men_1[movie_title] = word_counts_men_1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jObQ9HWTVNHv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jObQ9HWTVNHv",
    "outputId": "e54c3f31-2534-4c45-a957-2ab769bdcb86"
   },
   "outputs": [],
   "source": [
    "print(movies_dict_women_1)\n",
    "print(movies_dict_men_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d7231e-7a60-4d0d-afaa-162d6d0f4584",
   "metadata": {
    "id": "46d7231e-7a60-4d0d-afaa-162d6d0f4584"
   },
   "outputs": [],
   "source": [
    "\n",
    "movies_women = DataFrame.from_dict(movies_dict_women,orient=\"index\")\n",
    "\n",
    "movies_men = DataFrame.from_dict(movies_dict_men,orient=\"index\")\n",
    "\n",
    "print(movies_women.info())\n",
    "print('---------------------------------------------------------------------------')\n",
    "print(movies_men.info())\n",
    "\n",
    "\n",
    "total_desc_men = DataFrame.from_dict(movies_total_m, orient=\"index\")\n",
    "total_desc_women = DataFrame.from_dict(movies_total_f, orient=\"index\")\n",
    "\n",
    "total_desc_men\n",
    "# total_desc_women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "erG8sUSGt8eX",
   "metadata": {
    "id": "erG8sUSGt8eX"
   },
   "outputs": [],
   "source": [
    "# movies_women.to_csv(\"/content/drive/My Drive/DHDK/INFO-VIZ/infovizrepo/ProjectGaze/Data/movies_desc_women.csv\")\n",
    "# movies_men.to_csv(\"/content/drive/My Drive/DHDK/INFO-VIZ/infovizrepo/ProjectGaze/Data/movies_desc_men.csv\")\n",
    "\n",
    "total_desc_men.to_csv(\"/content/drive/My Drive/DHDK/INFO-VIZ/infovizrepo/ProjectGaze/Data/total_desc_men.csv\")\n",
    "total_desc_women.to_csv(\"/content/drive/My Drive/DHDK/INFO-VIZ/infovizrepo/ProjectGaze/Data/total_desc_women.csv\")\n",
    "\n",
    "print(movies_total_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5PPtDKAnog",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ca5PPtDKAnog",
    "outputId": "e8a5e3e9-a087-4959-b0bc-95db78b4b2ce"
   },
   "outputs": [],
   "source": [
    "    # Count total keywords across all movies\n",
    "for movie_title in movies_dict_women:\n",
    "    word_counts_women = movies_dict_women[movie_title]\n",
    "    for keyword in word_counts_women:\n",
    "        if keyword in movies_total_f:\n",
    "            movies_total_f[keyword] += word_counts_women[keyword]\n",
    "        else:\n",
    "            movies_total_f[keyword] = word_counts_women[keyword]\n",
    "\n",
    "for movie_title in movies_dict_men:\n",
    "    word_counts_men = movies_dict_men[movie_title]\n",
    "    for keyword in word_counts_men:\n",
    "        if keyword in movies_total_m:\n",
    "            movies_total_m[keyword] += word_counts_men[keyword]\n",
    "        else:\n",
    "            movies_total_m[keyword] = word_counts_men[keyword]\n",
    "\n",
    "\n",
    "print(movies_total_m)\n",
    "print(movies_total_f)\n",
    "\n",
    "\n",
    "\n",
    "    # Count total keywords across all movies INAPPROPRIATE SPEECH\n",
    "for movie_title in movies_dict_women_1:\n",
    "    word_counts_women_1 = movies_dict_women_1[movie_title]\n",
    "    for keyword in word_counts_women_1:\n",
    "        if keyword in movies_total_f_1:\n",
    "            movies_total_f_1[keyword] += word_counts_women_1[keyword]\n",
    "        else:\n",
    "            movies_total_f_1[keyword] = word_counts_women_1[keyword]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QNus8Dk8ZMnB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QNus8Dk8ZMnB",
    "outputId": "64274b1d-4dac-499b-ee12-6caee0748ae4"
   },
   "outputs": [],
   "source": [
    "for movie_title in movies_dict_men_1:\n",
    "    word_counts_men_1 = movies_dict_men_1[movie_title]\n",
    "    for keyword in word_counts_men_1:\n",
    "        if keyword in movies_total_m_1:\n",
    "            movies_total_m_1[keyword] += word_counts_men_1[keyword]\n",
    "        else:\n",
    "            movies_total_m_1[keyword] = word_counts_men_1[keyword]\n",
    "\n",
    "print(movies_dict_men_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_2kWVA4bPSzY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2235
    },
    "id": "_2kWVA4bPSzY",
    "outputId": "3abd392c-fa07-458e-84f7-6607aa49b091"
   },
   "outputs": [],
   "source": [
    "df_men = DataFrame.from_dict(movies_total_m, orient=\"index\")\n",
    "\n",
    "\n",
    "df_men = df_men.rename(columns={0:\"men_count\"})\n",
    "df_men\n",
    "\n",
    "df_women = DataFrame.from_dict(movies_total_f, orient=\"index\")\n",
    "\n",
    "\n",
    "df_women = df_women.rename(columns={0:\"women_count\"})\n",
    "df_women\n",
    "\n",
    "total_movie_counts = df_women.merge(df_men,left_index=True, right_index=True)\n",
    "total_movie_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ggcbPELIiYL0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ggcbPELIiYL0",
    "outputId": "410c7c01-23bb-48e6-a719-c343768cd9ba"
   },
   "outputs": [],
   "source": [
    "#word clouds\n",
    "\n",
    "%pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a word cloud image\n",
    "overall_count_women = movies_total_f\n",
    "overall_count_men = movies_total_m\n",
    "\n",
    "wordcloud = WordCloud(width = 1000, height = 1000, max_words=200).generate_from_frequencies(movies_total_f)\n",
    "wordcloud1 = WordCloud(width = 1000, height = 1000, max_words=200).generate_from_frequencies(movies_total_m)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(wordcloud)\n",
    "\n",
    "wordcloud_1_f= WordCloud(width = 1000, height = 1000, max_words=200).generate_from_frequencies(movies_total_f_1)\n",
    "wordcloud_1_m = WordCloud(width = 1000, height = 1000, max_words=200).generate_from_frequencies(movies_total_m_1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(wordcloud_1_f)\n",
    "# def gen_wc(dictionary):\n",
    "#   for key in dictionary.keys():\n",
    "#     wordcloud = WordCloud(width = 1000, height = 1000, max_words=200).generate_from_frequencies(key)\n",
    "#     plt.savefig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57495037-0bf0-4a8c-be21-441f9dc115d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "movies_dict #body descriptions\n",
    "movies_dict_1 #inappropriate\n",
    "\n",
    "#get total amount for each movie\n",
    "\n",
    "movie_stats = {movie: sum(words.values()) for movie, words in movies_dict.items()}\n",
    "\n",
    "movie_stats_inapp = {movie: sum(words.values()) for movie, words in movies_dict_1.items()}\n",
    "print(movie_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef28a3a-3914-49bc-a2fb-42917e9facc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
